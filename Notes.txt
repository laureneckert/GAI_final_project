Team Members:

Jaric Abadinas
Lauren Eckert

Topic: Use transfer learning to apply an artistic genre style to a personal photograph. The overall model will be CycleGAN, with the generator being ResNet and the discriminator being PatchGAN. We want to use Impressionist-landscapes-paintings from Robgonsalves art genre style dataset and Landscape Pictures from Arnaud Rougetet on Kaggle for the landscape photography dataset.

Environment set up:
- matplotlib
- tensorflow
- keras
- numpy
- opencv
- os
-----------------------------------------------------------------------------
Directory:

1. driver.py - The Main Script

This is the primary script that a user interacts with. It orchestrates the overall process, from loading the model to applying style transfer.

    main() Function:
        Manages the flow of the program.
        Handles the decision to either train a new model or load an existing one.
        Interacts with the user to get the path of the photograph for style transfer.
        Calls functions to apply the art style and display or save the output.

2. driverFunctions.py - Utility Functions for Data Handling and Processing

This file contains functions for image processing and handling user interactions.

    load_and_preprocess_image(image_path, target_size):
        Loads and preprocesses an image from a given path.
        Resizes the image to the target size and normalizes it.

    postprocess_image(image_tensor):
        Converts the output tensor from the model back into a standard image format.
        Rescales the pixel values and prepares the image for saving or displaying.

    apply_art_style(image_path, generator):
        Uses the provided generator model to apply an art style to an input image.
        Calls load_and_preprocess_image and postprocess_image for handling the image transformations.

    save_or_display_image(image, save, display, save_path):
        Saves or displays the styled image based on user preference.
        Handles file saving and image displaying functionalities.

3. MLmodel.py - Model Definitions and Training Logic

This file is the core of your machine learning model, containing the architecture of the CycleGAN and its training logic.

    build_generator():
        Defines and returns the architecture of the generator model.
        Utilizes a ResNet-based architecture for effective image translation.

    build_discriminator():
        Defines and returns the architecture of the discriminator model.
        Based on the PatchGAN design, suitable for distinguishing real and fake images at a patch level.

    compile_models(generator, discriminator, lr, beta_1):
        Compiles the generator and discriminator models with the specified optimizer and loss functions.

    train_model(art_images, photo_images, generator_AtoB, generator_BtoA, discriminator_A, discriminator_B, epochs, batch_size):
        Implements the training logic for the CycleGAN.
        Handles the training process over specified epochs and batch sizes, including forward and backward cycles, loss calculations, and weight updates.

    save_model(model, model_name, save_dir):
        Saves a trained model to a specified directory.

    load_model(model_name, model_dir):
        Loads a model from a specified directory.

-------------------------------------------------------------------
Program Flow:

    Starting the Program (driver.py):
        The user initiates the program by running driver.py. This triggers the main() function.

    Decision on Model Training or Loading:
        Inside main(), the program decides whether to train a new model or load a pre-trained model based on user input or predefined settings.
            If training a new model, it proceeds to load and preprocess datasets.
            If loading a pre-trained model, it skips to the style transfer step.

    Loading and Preprocessing Datasets (Training Scenario):
        If training is chosen, the load_datasets() function from driverFunctions.py is called to load and preprocess the ArtBench-10 and ImageNet datasets.

    Building and Compiling the Models:
        Still in the training scenario, the program calls build_generator() and build_discriminator() from MLmodel.py to create the CycleGAN models (two generators and two discriminators).
        These models are then compiled with appropriate loss functions and optimizers using compile_models() in MLmodel.py.

    Training the Models:
        The train_model() function in MLmodel.py is invoked to train the models. This function manages the entire training process, including forward and backward cycles, loss calculations, and updating the model weights.
        Upon completion of training, the trained generator model is saved using save_model() in MLmodel.py.

    Loading the Pre-trained Model (Inference Scenario):
        If the user opts to use a pre-trained model or if the training phase is complete, the program loads the generator model using load_model() in MLmodel.py.

    User Input for Style Transfer:
        The program then prompts the user (via driver.py) to input the path of a personal photograph for style transfer.

    Applying the Art Style:
        The apply_art_style() function in driverFunctions.py is called with the user's photograph and the loaded generator model. This function handles loading the image, preprocessing it, applying the style transfer, and then postprocessing the result.

    Saving or Displaying the Output:
        The transformed image is then either saved, displayed, or both, using the save_or_display_image() function in driverFunctions.py based on the user's preference or predefined settings.

    End of Program Execution:
        After the styled image is saved or displayed, the program concludes its execution.

-----------------------------------------------------------------

Training notes:

T0:

    # Optimizers with reduced learning rate
    gen_optimizer = Adam(learning_rate=0.0001, beta_1=0.5)
    disc_optimizer = Adam(learning_rate=0.0001, beta_1=0.5)

Epoch 1/10
        Step 1/100 - Time: 14.41s
                Generator A to B Loss: 3.0667
                Generator B to A Loss: 3.5346
                Discriminator A Loss: 1.6055
                Discriminator B Loss: 1.8901
        Step 11/100 - Time: 14.21s
                Generator A to B Loss: 3.2110
                Generator B to A Loss: 3.3221
                Discriminator A Loss: 1.7345
                Discriminator B Loss: 1.6976
        Step 21/100 - Time: 14.48s
                Generator A to B Loss: nan
                Generator B to A Loss: nan
                Discriminator A Loss: nan
                Discriminator B Loss: nan

gradient descent or something is getting out of control but why? doesn't resnet fix that?

T1:

PS C:\Users\laure> & C:/Users/laure/anaconda3/envs/tf_env_3/python.exe "c:/Users/laure/Dropbox/School/BSE/Coursework/23 Fall/GenerativeAI/code for projects/GAIfinalproject/GAI_final_project/driver.py"
Found 5000 images belonging to 1 classes.
Found 4319 images belonging to 1 classes.
2023-12-13 21:23:39.251463: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 21:23:39.252027: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Epoch 1/10
        Step 1/100 - Time: 14.40s
                Generator A to B Loss: 2.7686
                Generator B to A Loss: 2.6413
                Discriminator A Loss: 1.5854
                Discriminator B Loss: 1.5238
        Step 11/100 - Time: 14.13s
                Generator A to B Loss: nan
                Generator B to A Loss: nan
                Discriminator A Loss: nan
                Discriminator B Loss: nan
        Step 21/100 - Time: 15.80s
                Generator A to B Loss: nan
                Generator B to A Loss: nan
                Discriminator A Loss: nan
                Discriminator B Loss: nan
        Step 31/100 - Time: 14.16s
                Generator A to B Loss: nan
                Generator B to A Loss: nan
                Discriminator A Loss: nan
                Discriminator B Loss: nan
        Step 41/100 - Time: 14.17s
                Generator A to B Loss: nan
                Generator B to A Loss: nan
                Discriminator A Loss: nan
                Discriminator B Loss: nan
        Step 51/100 - Time: 14.17s
                Generator A to B Loss: nan
                Generator B to A Loss: nan
                Discriminator A Loss: nan
                Discriminator B Loss: nan
        Step 61/100 - Time: 14.12s
                Generator A to B Loss: nan
                Generator B to A Loss: nan
                Discriminator A Loss: nan
                Discriminator B Loss: nan


-----------------------------------------------------------------------------
TO DO:

