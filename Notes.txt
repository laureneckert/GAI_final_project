Team Members:

Jaric Abadinas
Lauren Eckert

Topic: Use transfer learning to apply an artistic genre style to a personal photograph. The overall model will be CycleGAN, with the generator and discriminator code provided from the GAN Cookbook CycleGAN tutorial. We want to use Impressionist-landscapes-paintings from Robgonsalves art genre style dataset and Landscape Pictures from Arnaud Rougetet on Kaggle for the landscape photography dataset. Both will be downloaded locally and loaded up in batches to preserve memory space.

-----------------------------------------------------------------------------
Directory:

a lot hAS CHANGED HERE

-------------------------------------------------------------------
Program Flow:

????

-----------------------------------------------------------------

Training notes:

T0:

    # Optimizers with reduced learning rate
    gen_optimizer = Adam(learning_rate=0.0001, beta_1=0.5)
    disc_optimizer = Adam(learning_rate=0.0001, beta_1=0.5)

Epoch 1/10
        Step 1/100 - Time: 14.41s
                Generator A to B Loss: 3.0667
                Generator B to A Loss: 3.5346
                Discriminator A Loss: 1.6055
                Discriminator B Loss: 1.8901
        Step 11/100 - Time: 14.21s
                Generator A to B Loss: 3.2110
                Generator B to A Loss: 3.3221
                Discriminator A Loss: 1.7345
                Discriminator B Loss: 1.6976
        Step 21/100 - Time: 14.48s
                Generator A to B Loss: nan
                Generator B to A Loss: nan
                Discriminator A Loss: nan
                Discriminator B Loss: nan

gradient descent or something is getting out of control but why? doesn't resnet fix that?

T1:

PS C:\Users\laure> & C:/Users/laure/anaconda3/envs/tf_env_3/python.exe "c:/Users/laure/Dropbox/School/BSE/Coursework/23 Fall/GenerativeAI/code for projects/GAIfinalproject/GAI_final_project/driver.py"
Found 5000 images belonging to 1 classes.
Found 4319 images belonging to 1 classes.
2023-12-13 21:23:39.251463: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 21:23:39.252027: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Epoch 1/10
        Step 1/100 - Time: 14.40s
                Generator A to B Loss: 2.7686
                Generator B to A Loss: 2.6413
                Discriminator A Loss: 1.5854
                Discriminator B Loss: 1.5238
        Step 11/100 - Time: 14.13s
                Generator A to B Loss: nan
                Generator B to A Loss: nan
                Discriminator A Loss: nan
                Discriminator B Loss: nan
        Step 21/100 - Time: 15.80s
                Generator A to B Loss: nan
                Generator B to A Loss: nan
                Discriminator A Loss: nan
                Discriminator B Loss: nan
        Step 31/100 - Time: 14.16s
                Generator A to B Loss: nan
                Generator B to A Loss: nan
                Discriminator A Loss: nan
                Discriminator B Loss: nan
        Step 41/100 - Time: 14.17s
                Generator A to B Loss: nan
                Generator B to A Loss: nan
                Discriminator A Loss: nan
                Discriminator B Loss: nan
        Step 51/100 - Time: 14.17s
                Generator A to B Loss: nan
                Generator B to A Loss: nan
                Discriminator A Loss: nan
                Discriminator B Loss: nan
        Step 61/100 - Time: 14.12s
                Generator A to B Loss: nan
                Generator B to A Loss: nan
                Discriminator A Loss: nan
                Discriminator B Loss: nan

T2:

okay we overhauled the model and used some code from GAN Cookbook so here is the terminal output now:

batch > 1

==================================================================================================
Total params: 44,614,724
Trainable params: 44,604,100
Non-trainable params: 10,624
__________________________________________________________________________________________________
Starting training process
Epoch 1/10

Processing batch 1 of epoch 1
Shape of batch_A: (10, 256, 256, 3)
Sample value from batch_A: [0.4745098  0.49019608 0.49803922]
Shape of batch_B: (10, 256, 256, 3)
Sample value from batch_B: [-0.04313726  0.1764706  -0.00392157]
Log 1
Log 2
Shape of fake_A: (10, 256, 256, 3)
Sample value from fake_A: [-0.02410066  0.03852579 -0.0046167 ]
Training discriminator with real data...

then it get stuck there forever even for batch = 2, which should be manageable

batch = 1
==================================================================================================
Total params: 44,614,724
Trainable params: 44,604,100
Non-trainable params: 10,624
__________________________________________________________________________________________________
Starting training process
Epoch 1/10

Processing batch 1 of epoch 1
Shape of batch_A: (1, 256, 256, 3)
Sample value from batch_A: [0.4745098  0.49019608 0.49803922]
Shape of batch_B: (1, 256, 256, 3)
Sample value from batch_B: [-0.04313726  0.1764706  -0.00392157]
Log 1
Log 2
Shape of fake_A: (1, 256, 256, 3)
Sample value from fake_A: [0.02611968 0.00967421 0.00206762]
Training discriminator with real data...
Log 3 - Discriminator real
Training discriminator with fake data...
Log 4 - Discriminator fake
Log 5 - Generator
Batch 1: [Discriminator Loss: nan], [Generator Loss: [nan, nan, 0.6247756481170654]]
Checkpoint reached. Checkpoint saving skipped.

Processing batch 2 of epoch 1
Shape of batch_A: (1, 256, 256, 3)
Sample value from batch_A: [0.4745098  0.49019608 0.49803922]
Shape of batch_B: (1, 256, 256, 3)
Sample value from batch_B: [-0.04313726  0.1764706  -0.00392157]
Log 1
Log 2
Shape of fake_A: (1, 256, 256, 3)
Sample value from fake_A: [-1. -1. -1.]
Training discriminator with real data...
Log 3 - Discriminator real
Training discriminator with fake data...
Log 4 - Discriminator fake
Log 5 - Generator
Batch 2: [Discriminator Loss: nan], [Generator Loss: [nan, nan, 1.1008976697921753]]

Processing batch 3 of epoch 1
Shape of batch_A: (1, 256, 256, 3)
Sample value from batch_A: [0.4745098  0.49019608 0.49803922]
Shape of batch_B: (1, 256, 256, 3)
Sample value from batch_B: [-0.04313726  0.1764706  -0.00392157]
Log 1
Log 2
Shape of fake_A: (1, 256, 256, 3)
Sample value from fake_A: [-1. -1. -1.]
Training discriminator with real data...
Log 3 - Discriminator real
Training discriminator with fake data...
Log 4 - Discriminator fake
Log 5 - Generator
Batch 3: [Discriminator Loss: nan], [Generator Loss: [nan, nan, 1.1008976697921753]]

Processing batch 4 of epoch 1
Shape of batch_A: (1, 256, 256, 3)
Sample value from batch_A: [0.4745098  0.49019608 0.49803922]
Shape of batch_B: (1, 256, 256, 3)
Sample value from batch_B: [-0.04313726  0.1764706  -0.00392157]
Log 1
Log 2
Shape of fake_A: (1, 256, 256, 3)
Sample value from fake_A: [-1. -1. -1.]
Training discriminator with real data...
Log 3 - Discriminator real
Training discriminator with fake data...
Log 4 - Discriminator fake
Log 5 - Generator
Batch 4: [Discriminator Loss: nan], [Generator Loss: [nan, nan, 1.1008976697921753]]

Processing batch 5 of epoch 1
Shape of batch_A: (1, 256, 256, 3)
Sample value from batch_A: [0.4745098  0.49019608 0.49803922]
Shape of batch_B: (1, 256, 256, 3)
Sample value from batch_B: [-0.04313726  0.1764706  -0.00392157]
Log 1
Log 2
Shape of fake_A: (1, 256, 256, 3)
Sample value from fake_A: [-1. -1. -1.]
Training discriminator with real data...
Log 3 - Discriminator real
Training discriminator with fake data...
Log 4 - Discriminator fake
Log 5 - Generator
Batch 5: [Discriminator Loss: nan], [Generator Loss: [nan, nan, 1.1008976697921753]]

Processing batch 6 of epoch 1
Shape of batch_A: (1, 256, 256, 3)
Sample value from batch_A: [0.4745098  0.49019608 0.49803922]
Shape of batch_B: (1, 256, 256, 3)
Sample value from batch_B: [-0.04313726  0.1764706  -0.00392157]
Log 1
Log 2
Shape of fake_A: (1, 256, 256, 3)
Sample value from fake_A: [-1. -1. -1.]
Training discriminator with real data...
Log 3 - Discriminator real
Training discriminator with fake data...
Log 4 - Discriminator fake
Log 5 - Generator
Batch 6: [Discriminator Loss: nan], [Generator Loss: [nan, nan, 1.1008976697921753]]

Processing batch 7 of epoch 1
Shape of batch_A: (1, 256, 256, 3)
Sample value from batch_A: [0.4745098  0.49019608 0.49803922]
Shape of batch_B: (1, 256, 256, 3)
Sample value from batch_B: [-0.04313726  0.1764706  -0.00392157]
Log 1
Log 2
Shape of fake_A: (1, 256, 256, 3)
Sample value from fake_A: [-1. -1. -1.]
Training discriminator with real data...
Log 3 - Discriminator real
Training discriminator with fake data...
Log 4 - Discriminator fake
Log 5 - Generator
Batch 7: [Discriminator Loss: nan], [Generator Loss: [nan, nan, 1.1008976697921753]]

Processing batch 8 of epoch 1
Shape of batch_A: (1, 256, 256, 3)
Sample value from batch_A: [0.4745098  0.49019608 0.49803922]
Shape of batch_B: (1, 256, 256, 3)
Sample value from batch_B: [-0.04313726  0.1764706  -0.00392157]
Log 1
Log 2
Shape of fake_A: (1, 256, 256, 3)
Sample value from fake_A: [-1. -1. -1.]
Training discriminator with real data...
Log 3 - Discriminator real
Training discriminator with fake data...
Log 4 - Discriminator fake
Log 5 - Generator
Batch 8: [Discriminator Loss: nan], [Generator Loss: [nan, nan, 1.1008976697921753]]

Processing batch 9 of epoch 1
Shape of batch_A: (1, 256, 256, 3)
Sample value from batch_A: [0.4745098  0.49019608 0.49803922]
Shape of batch_B: (1, 256, 256, 3)
Sample value from batch_B: [-0.04313726  0.1764706  -0.00392157]
Log 1
Log 2
Shape of fake_A: (1, 256, 256, 3)
Sample value from fake_A: [-1. -1. -1.]
Training discriminator with real data...
Log 3 - Discriminator real
Training discriminator with fake data...
Log 4 - Discriminator fake
Log 5 - Generator
Batch 9: [Discriminator Loss: nan], [Generator Loss: [nan, nan, 1.1008976697921753]]

Processing batch 10 of epoch 1
Shape of batch_A: (1, 256, 256, 3)
Sample value from batch_A: [0.4745098  0.49019608 0.49803922]
Shape of batch_B: (1, 256, 256, 3)
Sample value from batch_B: [-0.04313726  0.1764706  -0.00392157]
Log 1
Log 2

....and so on. obvi not useful beccause of the NAN loss values

-----------------------------------------------------------------------------
TO DO:

figure out this fucking disaster :((((((

why wont the discriminator work!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! the generator is working fine I THINK
